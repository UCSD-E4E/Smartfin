{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smartfin Website Scrape\n",
    "Smartfin ride data can be accessed through the Smartfin website through ride ids. Each Smartfin session has a unique ride id that we use to access the data of that session. The data to be accessed are the motion data and ocean data\n",
    "\n",
    "Motion Data is data collected by the Smartfin's IMU sensors and take measurements of acceleration, rotation, and orientaion, each in the x, y, and z axis respectively.\n",
    "\n",
    "Ocean Data records things like temperature, salinity, and pH, although the current generation of Smartfin only records temperature\n",
    "\n",
    "The code in this notebook is used to add new smartfin session data to the Ride API database, so you won't actually have to web scrape any data manually for this project unless you just want to play around with the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rid:  15692\n",
      "ride_url: https://surf.smartfin.org/ride/15692\n",
      "Ride threw an exception!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a740a372b398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;31m# concatinate all dataframes in each list into one big dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m \u001b[0mocean_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappended_ocean_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ride_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[0mmotion_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappended_motion_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ride_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \"\"\"\n\u001b[1;32m--> 271\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc(\"font\", size=14) \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "import peakutils\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "ride_ids = ['15692']\n",
    "\n",
    "\n",
    "#%% Fin ID scraper\n",
    "# Input fin ID, get all ride IDs\n",
    "# base URL to which we'll append given fin IDs\n",
    "# fin_url_base = 'http://surf.smartfin.org/fin/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "# str_id_ride = 'rideId = \\'' # backslash allows us to look for single quote\n",
    "# str_id_date = 'var date = \\'' # backslash allows us to look for single quote\n",
    "\n",
    "#%% Ride ID scraper\n",
    "\n",
    "\n",
    "# Input ride ID, get ocean and motion CSVs\n",
    "# Base URL to which we'll append given ride IDs\n",
    "ride_url_base = 'https://surf.smartfin.org/ride/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below to get csv id \n",
    "str_id_csv = 'img id=\"temperatureChart\" class=\"chart\" src=\"' \n",
    "\n",
    "\n",
    "def get_csv_from_ride_id(rid):\n",
    "    \n",
    "# step 1    \n",
    "    # Build URL for each individual ride\n",
    "    ride_url = ride_url_base+str(rid)\n",
    "    print(\"ride_url: \" + ride_url)\n",
    "    \n",
    "# step 2\n",
    "    # query smartfin website to retrieve the ride's webpage in HTML  \n",
    "    html_contents = requests.get(ride_url).text\n",
    "#     print(\"html contents: \", html_contents)\n",
    "    \n",
    "    # Find CSV file location id in html page by csv file tag\n",
    "    loc_csv_id = html_contents.find(str_id_csv)\n",
    "#     print(\"loc_csv_id: \", loc_csv_id)\n",
    "    \n",
    "# step 3\n",
    "    # log into smartfin website to get request authentication\n",
    "    # Different based on whether user logged in with FB or Google\n",
    "    offset_googleOAuth = [46, 114]\n",
    "    offset_facebkOAuth = [46, 112]\n",
    "    if html_contents[loc_csv_id+59] == 'f': # Facebook login\n",
    "        off0 = offset_facebkOAuth[0]\n",
    "        off1 = offset_facebkOAuth[1]\n",
    "    else: # Google login\n",
    "        off0 = offset_googleOAuth[0]\n",
    "        off1 = offset_googleOAuth[1]\n",
    "\n",
    "# step 4\n",
    "    # use csv id and authentication offsets to build query string\n",
    "    csv_id_longstr = html_contents[loc_csv_id+off0:loc_csv_id+off1]\n",
    "#     print(\"csv_id_longstr: \", csv_id_longstr)\n",
    "    \n",
    "    # Stitch together full URL for CSV\n",
    "    if (\"media\" in csv_id_longstr) & (\"Calibration\" not in html_contents): # other junk URLs can exist and break everything\n",
    "\n",
    "# step 5\n",
    "        # full urls to get csv file       \n",
    "        ocean_csv_url = f'https://surf.smartfin.org/{csv_id_longstr}Ocean.CSV'\n",
    "        motion_csv_url = f'https://surf.smartfin.org/{csv_id_longstr}Motion.CSV'\n",
    "        \n",
    "        print(\"ocean_csv_url: \", ocean_csv_url)\n",
    "        print(\"motion_csv_url: \", motion_csv_url)\n",
    "        print('\\n\\n')\n",
    "\n",
    "# step 6\n",
    "        # Go to ocean_csv_url and grab contents (theoretically, a CSV)\n",
    "        ocean_df_small = pd.read_csv(ocean_csv_url, parse_dates = [0])\n",
    "        motion_df_small = pd.read_csv(motion_csv_url, parse_dates = [0])\n",
    "\n",
    "\n",
    "# step 7\n",
    "        # 7a. add elasped column to show how much time has elapsed since first reading\n",
    "        elapsed_timedelta = (ocean_df_small['UTC']-ocean_df_small['UTC'][0])\n",
    "        ocean_df_small['elapsed'] = elapsed_timedelta/np.timedelta64(1, 's')\n",
    "        \n",
    "        print(\"motion_df_small raw: \", motion_df_small)\n",
    "        print('\\n\\n')\n",
    "        print(\"ocean_df_small raw: \", ocean_df_small)\n",
    "        print('\\n\\n')\n",
    "\n",
    "\n",
    "        \n",
    "        # 7b. make the index of each df the timestamp\n",
    "        if len(ocean_df_small) > 1:\n",
    "            ocean_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            motion_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            \n",
    "            print(\"ocean_df_small length pre upsample: \", len(ocean_df_small))\n",
    "            print(\"motion_df_small length pre upsample: \", len(motion_df_small))\n",
    "            \n",
    "            # 7c. resample data to 33ms intervals (30 Hz)\n",
    "            #May need to change this sampling interval:\n",
    "            sample_interval = '1000ms'\n",
    "                        \n",
    "            ocean_df_small_resample = ocean_df_small.resample(sample_interval).mean()\n",
    "            motion_df_small_resample = motion_df_small.resample(sample_interval).mean()\n",
    "    \n",
    "            \n",
    "            print('ocean_df_resample length: ', len(ocean_df_small_resample))\n",
    "            print('motion_df_resample length: ', len(motion_df_small_resample))\n",
    "            print('\\n\\n')\n",
    "            \n",
    "            # returns all rows that have values in latitude column           \n",
    "            # No need to save many extra rows with no fix\n",
    "            # motion_df_small = motion_df_small[~np.isnan(motion_df_small.Latitude)]\n",
    "            \n",
    "            return ocean_df_small_resample, motion_df_small_resample\n",
    "\n",
    "    # if dataframe is empty, just return empty dataframe    \n",
    "    else:\n",
    "        ocean_df_small_resample = pd.DataFrame() # empty DF just so something is returned\n",
    "        motion_df_small_resample = pd.DataFrame() \n",
    "        return ocean_df_small_resample, motion_df_small_resample\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# actual script\n",
    "\n",
    "appended_ocean_list = [] # list of DataFrames from original CSVs\n",
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "## Nested loops (for each fin ID, find all ride IDs, then build a DataFrame from all ride CSVs)\n",
    "## (Here, ride IDS are either ocean or motion dataframes)\n",
    "count_good_fins = 0      # number of dataframes with non empty data\n",
    "    \n",
    "# Loop over ride_ids and find CSVs\n",
    "for rid in ride_ids:\n",
    "    print(\"rid: \", rid)\n",
    "    try:\n",
    "        # runs code from function defined above\n",
    "        new_ocean_df, new_motion_df = get_csv_from_ride_id(rid) # get given ride's CSV from its ride ID using function above\n",
    "        \n",
    "        # for each non empty df, append to list of already created dataframes        \n",
    "        if not new_ocean_df.empty: # Calibration rides, for example\n",
    "            \n",
    "            # Append only if DF isn't empty. There may be a better way to control empty DFs which are created above\n",
    "            appended_multiIndex.append(str(rid)) # build list to be multiIndex of future DataFrame\n",
    "            appended_ocean_list.append(new_ocean_df)\n",
    "            appended_motion_list.append(new_motion_df)\n",
    "            \n",
    "            count_good_fins += 1\n",
    "        \n",
    "    except: \n",
    "        print(\"Ride threw an exception!\")\n",
    "        #print(\"Ride \", rid, \"threw an exception!\")    \n",
    "\n",
    "#%% Build the \"Master\" DataFrame\n",
    "# keys for each diferent dataframe in the big dataframes\n",
    "df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, a list which data in it cannot be changed\n",
    "\n",
    "# concatinate all dataframes in each list into one big dataframe\n",
    "ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id'])\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "\n",
    "##Here, maybe just use info from the motion_df and don't worry about ocean_df data for now.\n",
    "##If you do want ocean_df data, look at how Phil was getting it from \"July 10th and 11th Calibration\" jupyter notebook file.\n",
    "#We can also check to see if the surfboard was recording \"in-water-freq\" or \n",
    "#\"out-of-water-freq\" based on how many NaN values we see. \n",
    "\n",
    "\n",
    "\n",
    "# 7d. clear na values from dataframes\n",
    "#Drop the latitude and longitude values since most of them are Nan:\n",
    "print('motion df length pre na drop: ', len(motion_df))\n",
    "motion_df_dropped = motion_df.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "#Drop the NAN values from the motion data:\n",
    "motion_df_dropped = motion_df_dropped.dropna(axis=0, how='any')\n",
    "print('motion_df_dropped length post na drop: ', len(motion_df_dropped))\n",
    "print('\\n\\n')\n",
    "\n",
    "# finished clean dataframes\n",
    "print('motion_df_dropped: ', motion_df_dropped)\n",
    "print('ocean_df: ', ocean_df)\n",
    "motion_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motion_df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a pandas dataframe to zipped CSV file\n",
    "motion_df_dropped.to_csv(\"motion_dataframes/15692.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a pandas dataframe to zipped CSV file\n",
    "motion_df_dropped.to_csv(\"motion_dataframes/15692.csv.gzip\", \n",
    "           index=False,\n",
    "           compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df_dropped.to_csv(\"motion_dataframes/15692.csv.zip\", \n",
    "           index=False,\n",
    "           compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('motion_dataframes/15692.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
